{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_320x320.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Vr7JQPTq_qla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Approach\n",
        "\n",
        "* Firstly a convolutional neural network is used to segment the image, using the bounding boxes directly as a mask. \n",
        "* Secondly connected components is used to separate multiple areas of predicted pneumonia.\n",
        "* Finally a bounding box is simply drawn around every connected component.\n",
        "\n",
        "# Network\n",
        "\n",
        "* The network consists of a number of residual blocks with convolutions and downsampling blocks with max pooling.\n",
        "* At the end of the network a single upsampling layer converts the output to the same shape as the input.\n",
        "\n",
        "As the input to the network is 256 by 256 (instead of the original 1024 by 1024) and the network downsamples a number of times without any meaningful upsampling (the final upsampling is just to match in 256 by 256 mask) the final prediction is very crude. If the network downsamples 4 times the final bounding boxes can only change with at least 16 pixels.\n",
        "\n",
        "**Edit by EAS** - Change input image size to 320x320."
      ]
    },
    {
      "metadata": {
        "id": "5jFPqYbl_tWH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip install -q pydicom \n",
        "!pip install -q imgaug \n",
        "!pip install -q kaggle\n",
        "!pip install -q tqdm \n",
        "!pip install -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcb2HG8n_qlb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import measure\n",
        "from skimage.transform import resize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvAZDt6q_ukM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "    media = MediaFileUpload(path, \n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7FbV28E_v4N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# enter your Kaggle credentionals here\n",
        "os.environ['KAGGLE_USERNAME']=\"skooch\"\n",
        "os.environ['KAGGLE_KEY']=\"42f8a02ee92cc773d1dbe66565673ad3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCkxN_zR_xHH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you are unable to download the competition dataset, check to see if you have \n",
        "# accepted the user agreement on the competition website. \n",
        "if not os.path.exists(\"./stage_1_detailed_class_info.csv.zip\"):\n",
        "    !kaggle competitions download -c rsna-pneumonia-detection-challenge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f2IeRx6z_yTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unzipping takes a few minutes\n",
        "if not os.path.exists(\"./stage_1_train_labels.csv\"):\n",
        "    print(\"Unzipping...\")\n",
        "    !unzip -q -o stage_1_test_images.zip -d stage_1_test_images\n",
        "    !unzip -q -o stage_1_train_images.zip -d stage_1_train_images\n",
        "    !unzip -q -o stage_1_train_labels.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltOidyEU_qle",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load pneumonia locations\n",
        "\n",
        "Table contains [filename : pneumonia location] pairs per row. \n",
        "* If a filename contains multiple pneumonia, the table contains multiple rows with the same filename but different pneumonia locations. \n",
        "* If a filename contains no pneumonia it contains a single row with an empty pneumonia location.\n",
        "\n",
        "The code below loads the table and transforms it into a dictionary. \n",
        "* The dictionary uses the filename as key and a list of pneumonia locations in that filename as value. \n",
        "* If a filename is not present in the dictionary it means that it contains no pneumonia."
      ]
    },
    {
      "metadata": {
        "id": "bJaASoWC_qle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# empty dictionary\n",
        "pneumonia_locations = {}\n",
        "# load table\n",
        "with open(os.path.join('./stage_1_train_labels.csv'), mode='r') as infile:\n",
        "    # open reader\n",
        "    reader = csv.reader(infile)\n",
        "    # skip header\n",
        "    next(reader, None)\n",
        "    # loop through rows\n",
        "    for rows in reader:\n",
        "        # retrieve information\n",
        "        filename = rows[0]\n",
        "        location = rows[1:5]\n",
        "        pneumonia = rows[5]\n",
        "        # if row contains pneumonia add label to dictionary\n",
        "        # which contains a list of pneumonia locations per filename\n",
        "        if pneumonia == '1':\n",
        "            # convert string to float to int\n",
        "            location = [int(float(i)) for i in location]\n",
        "            # save pneumonia location in dictionary\n",
        "            if filename in pneumonia_locations:\n",
        "                pneumonia_locations[filename].append(location)\n",
        "            else:\n",
        "                pneumonia_locations[filename] = [location]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2yLhAUNl_qli",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load filenames"
      ]
    },
    {
      "metadata": {
        "id": "mG2EOT-8_qlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0dfb432-7655-46f7-97ad-80f588cf3aa7"
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "random.seed(1977)\n",
        "folder = './stage_1_train_images'\n",
        "filenames = os.listdir(folder)\n",
        "random.shuffle(filenames)\n",
        "# split into train and validation filenames\n",
        "n_valid_samples = 2560\n",
        "train_filenames = filenames[n_valid_samples:]\n",
        "valid_filenames = filenames[:n_valid_samples]\n",
        "print('n train samples', len(train_filenames))\n",
        "print('n valid samples', len(valid_filenames))\n",
        "n_train_samples = len(filenames) - n_valid_samples"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n train samples 23124\n",
            "n valid samples 2560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vMmvXsFS_qlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # Data generator\n",
        "\n",
        "The dataset is too large to fit into memory, so we need to create a generator that loads data on the fly.\n",
        "\n",
        "* The generator takes in some filenames, batch_size and other parameters.\n",
        "\n",
        "* The generator outputs a random batch of numpy images and numpy masks.\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "Z3SDWRfvAH4l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "IMAGE_SIZE = 320\n",
        "CHECKPOINT_PATH = \"model9_320.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSwVxQKdBB3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# upload checkpoint to GCS\n",
        "project_id = 'mammography-198911'\n",
        "bucket_name = 'pneumonia'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil cp gs://{bucket_name}/{CHECKPOINT_PATH} ./{CHECKPOINT_PATH} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKZdiz76_qlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class generator(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=True, augment=False, predict=False):\n",
        "        self.folder = folder\n",
        "        self.filenames = filenames\n",
        "        self.pneumonia_locations = pneumonia_locations\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.predict = predict\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __load__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # create empty mask\n",
        "        msk = np.zeros(img.shape)\n",
        "        # get filename without extension\n",
        "        filename = filename.split('.')[0]\n",
        "        # if image contains pneumonia\n",
        "        if filename in pneumonia_locations:\n",
        "            # loop through pneumonia\n",
        "            for location in pneumonia_locations[filename]:\n",
        "                # add 1's at the location of the pneumonia\n",
        "                x, y, w, h = location\n",
        "                msk[y:y+h, x:x+w] = 1\n",
        "        # if augment then horizontal flip half the time\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            img = np.fliplr(img)\n",
        "            msk = np.fliplr(msk)\n",
        "        # resize both image and mask\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        msk = np.expand_dims(msk, -1)\n",
        "        return img, msk\n",
        "    \n",
        "    def __loadpredict__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # resize image\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        return img\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # select batch\n",
        "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # predict mode: return images and filenames\n",
        "        if self.predict:\n",
        "            # load files\n",
        "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            return imgs, filenames\n",
        "        # train mode: return images and masks\n",
        "        else:\n",
        "            # load files\n",
        "            items = [self.__load__(filename) for filename in filenames]\n",
        "            # unzip images and masks\n",
        "            imgs, msks = zip(*items)\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            msks = np.array(msks)\n",
        "            return imgs, msks\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.filenames)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.predict:\n",
        "            # return everything\n",
        "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "        else:\n",
        "            # return full batches only\n",
        "            return int(len(self.filenames) / self.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuDIxMEW_qlt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Network"
      ]
    },
    {
      "metadata": {
        "id": "2cENEbF7_qlu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_downsample(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.MaxPool2D(2)(x)\n",
        "    return x\n",
        "\n",
        "def create_resblock(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    return keras.layers.add([x, inputs])\n",
        "\n",
        "def create_network(input_size, channels, n_blocks=2, depth=4):\n",
        "    # input\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n",
        "    # residual blocks\n",
        "    for d in range(depth):\n",
        "        channels = channels * 2\n",
        "        x = create_downsample(channels, x)\n",
        "        for b in range(n_blocks):\n",
        "            x = create_resblock(channels, x)\n",
        "    # output\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    outputs = keras.layers.UpSampling2D(2**depth)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NCUyPB3i_qlx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train network\n"
      ]
    },
    {
      "metadata": {
        "id": "QkzK5IBY_qly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2941
        },
        "outputId": "31cf2d20-1b5e-440b-9e3b-96ab40e90d10"
      },
      "cell_type": "code",
      "source": [
        "# define iou or jaccard loss function\n",
        "def iou_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
        "    return 1 - score\n",
        "\n",
        "# combine bce loss and iou loss\n",
        "def iou_bce_loss(y_true, y_pred):\n",
        "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
        "\n",
        "# mean iou as a metric\n",
        "def mean_iou(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
        "    smooth = tf.ones(tf.shape(intersect))\n",
        "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
        "\n",
        "# mean iou with any image with no truth counted as 0\n",
        "def adj_mean_iou(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    y_true_pos = tf.reduce_max(y_true, axis=[1, 2, 3])\n",
        "    \n",
        "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
        "    smooth = tf.ones(tf.shape(intersect))\n",
        "    \n",
        "    return tf.reduce_mean((y_true_pos * (intersect + smooth)) / (union - intersect + smooth))      \n",
        "  \n",
        "# create network and compiler\n",
        "model = create_network(input_size=320, channels=32, n_blocks=2, depth=4)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=iou_bce_loss,\n",
        "              metrics=['accuracy', mean_iou, adj_mean_iou])\n",
        "\n",
        "# cosine learning rate annealing\n",
        "def cosine_annealing(x):\n",
        "    lr = 0.002\n",
        "    epochs = 30\n",
        "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
        "  \n",
        "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n",
        "\n",
        "# create train and validation generators\n",
        "folder = './stage_1_train_images'\n",
        "train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=True, augment=True, predict=False)\n",
        "valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=False, predict=False)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 320, 320, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 320, 320, 32) 288         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 320, 320, 32) 128         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 320, 320, 32) 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 320, 320, 64) 2048        leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 160, 160, 64) 0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 160, 160, 64) 256         max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 160, 160, 64) 256         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 160, 160, 64) 0           conv2d_25[0][0]                  \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 160, 160, 64) 256         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 160, 160, 64) 256         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 160, 160, 64) 36864       leaky_re_lu_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 160, 160, 64) 0           conv2d_27[0][0]                  \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 160, 160, 64) 256         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)      (None, 160, 160, 64) 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 160, 160, 128 8192        leaky_re_lu_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 80, 80, 128)  0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 80, 80, 128)  512         max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 80, 80, 128)  512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 80, 80, 128)  0           conv2d_30[0][0]                  \n",
            "                                                                 max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 80, 80, 128)  512         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 80, 80, 128)  512         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 80, 80, 128)  147456      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 80, 80, 128)  0           conv2d_32[0][0]                  \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 80, 80, 128)  512         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 80, 80, 128)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 80, 80, 256)  32768       leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 40, 40, 256)  0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 40, 40, 256)  1024        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 40, 40, 256)  1024        conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 40, 40, 256)  0           conv2d_35[0][0]                  \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 40, 40, 256)  1024        add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 40, 40, 256)  1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 40, 40, 256)  589824      leaky_re_lu_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 40, 40, 256)  0           conv2d_37[0][0]                  \n",
            "                                                                 add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 40, 40, 256)  1024        add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)      (None, 40, 40, 256)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 40, 40, 512)  131072      leaky_re_lu_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 20, 20, 512)  0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 20, 20, 512)  2048        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 20, 20, 512)  2048        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_38 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 20, 20, 512)  0           conv2d_40[0][0]                  \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 20, 20, 512)  2048        add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_39 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 20, 20, 512)  2048        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_40 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 20, 20, 512)  2359296     leaky_re_lu_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 20, 20, 512)  0           conv2d_42[0][0]                  \n",
            "                                                                 add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 20, 20, 512)  2048        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_41 (LeakyReLU)      (None, 20, 20, 512)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 20, 20, 1)    513         leaky_re_lu_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 320, 320, 1)  0           conv2d_43[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 12,727,969\n",
            "Trainable params: 12,718,305\n",
            "Non-trainable params: 9,664\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G369kKMh_ql2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "644b3888-da36-4d84-90db-471832006cc2"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=15, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1445/1445 [==============================] - 1328s 919ms/step - loss: 0.5030 - acc: 0.9600 - mean_iou: 0.5697 - adj_mean_iou: 0.0336 - val_loss: 0.4766 - val_acc: 0.9562 - val_mean_iou: 0.5023 - val_adj_mean_iou: 0.0596\n",
            "Epoch 2/15\n",
            "1445/1445 [==============================] - 1316s 911ms/step - loss: 0.4648 - acc: 0.9649 - mean_iou: 0.6280 - adj_mean_iou: 0.0500 - val_loss: 0.4601 - val_acc: 0.9691 - val_mean_iou: 0.6970 - val_adj_mean_iou: 0.0458\n",
            "Epoch 3/15\n",
            "1445/1445 [==============================] - 1324s 916ms/step - loss: 0.4493 - acc: 0.9673 - mean_iou: 0.6603 - adj_mean_iou: 0.0548 - val_loss: 0.4546 - val_acc: 0.9589 - val_mean_iou: 0.6382 - val_adj_mean_iou: 0.0639\n",
            "Epoch 4/15\n",
            "1445/1445 [==============================] - 1325s 917ms/step - loss: 0.4420 - acc: 0.9680 - mean_iou: 0.6650 - adj_mean_iou: 0.0579 - val_loss: 0.4610 - val_acc: 0.9729 - val_mean_iou: 0.7318 - val_adj_mean_iou: 0.0372\n",
            "Epoch 5/15\n",
            "1445/1445 [==============================] - 1323s 916ms/step - loss: 0.4344 - acc: 0.9689 - mean_iou: 0.6752 - adj_mean_iou: 0.0607 - val_loss: 0.4258 - val_acc: 0.9708 - val_mean_iou: 0.7117 - val_adj_mean_iou: 0.0593\n",
            "Epoch 6/15\n",
            "1445/1445 [==============================] - 1320s 913ms/step - loss: 0.4288 - acc: 0.9694 - mean_iou: 0.6799 - adj_mean_iou: 0.0622 - val_loss: 0.4389 - val_acc: 0.9583 - val_mean_iou: 0.5756 - val_adj_mean_iou: 0.0790\n",
            "Epoch 7/15\n",
            "1066/1445 [=====================>........] - ETA: 5:10 - loss: 0.4234 - acc: 0.9697 - mean_iou: 0.6811 - adj_mean_iou: 0.0653"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fCE_u2jq_ql4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "327bf047-59d6-48ae-96df-10cf3720a4cf"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(131)\n",
        "plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
        "plt.legend()\n",
        "plt.subplot(132)\n",
        "plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
        "plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
        "plt.legend()\n",
        "plt.subplot(133)\n",
        "plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
        "plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c75bcf695eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m131\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Valid loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO8AAAD8CAYAAACfMvOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f554bbc8550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zpfjJkeuApN9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sample Predictions"
      ]
    },
    {
      "metadata": {
        "id": "vUKPS-jCAqTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.5\n",
        "# look at some sample predictions\n",
        "samples = np.random.choice(valid_filenames, size=8, replace=False)\n",
        "\n",
        "for filename in samples:\n",
        "    # load the image\n",
        "    img = pydicom.dcmread(os.path.join(train_dicom_dir, filename)).pixel_array\n",
        "    \n",
        "    msk = np.zeros(img.shape)\n",
        "    \n",
        "    filename = filename.split('.')[0]\n",
        "    # if image contains pneumonia\n",
        "    if filename in pneumonia_locations:\n",
        "        # loop through pneumonia\n",
        "        for location in pneumonia_locations[filename]:\n",
        "            # add 1's at the location of the pneumonia\n",
        "            x, y, w, h = location\n",
        "            msk[y:y+h, x:x+w] = 1\n",
        "        \n",
        "    # resize both image and mask\n",
        "    msk = resize(msk, (IMAGE_SIZE, IMAGE_SIZE), mode='reflect') > 0.5\n",
        "    img = resize(img, (IMAGE_SIZE, IMAGE_SIZE), mode='reflect')\n",
        "    \n",
        "    # predict the image\n",
        "    yhat = model.predict(img.reshape(1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "    yhat = yhat > THRESHOLD\n",
        "    # add trailing channel dimension\n",
        "    msk = np.expand_dims(msk, -1)\n",
        "\n",
        "    f, ax = plt.subplots(1, 3, figsize=(10, 4))\n",
        "    ax[0].imshow(img.reshape(IMAGE_SIZE,IMAGE_SIZE))\n",
        "    ax[0].set_title(\"Image\")\n",
        "    ax[1].imshow(msk.reshape(IMAGE_SIZE,IMAGE_SIZE))\n",
        "    ax[1].set_title(\"Label\")\n",
        "    ax[2].imshow(yhat.reshape(IMAGE_SIZE,IMAGE_SIZE))\n",
        "    ax[2].set_title(\"Prediction\")\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Att9nue5_ql7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict test images"
      ]
    },
    {
      "metadata": {
        "id": "F6-gG8q__ql7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "folder = '../input/stage_1_test_images'\n",
        "test_filenames = os.listdir(folder)\n",
        "print('n test samples:', len(test_filenames))\n",
        "\n",
        "# create test generator with predict flag set to True\n",
        "test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=320, shuffle=False, predict=True)\n",
        "\n",
        "# create submission dictionary\n",
        "submission_dict = {}\n",
        "# loop through testset\n",
        "for imgs, filenames in test_gen:\n",
        "    # predict batch of images\n",
        "    preds = model.predict(imgs)\n",
        "    # loop through batch\n",
        "    for pred, filename in zip(preds, filenames):\n",
        "        # resize predicted mask\n",
        "        pred = resize(pred, (1024, 1024), mode='reflect')\n",
        "        # threshold predicted mask\n",
        "        comp = pred[:, :, 0] > 0.5\n",
        "        # apply connected components\n",
        "        comp = measure.label(comp)\n",
        "        # apply bounding boxes\n",
        "        predictionString = ''\n",
        "        for region in measure.regionprops(comp):\n",
        "            # retrieve x, y, height and width\n",
        "            y, x, y2, x2 = region.bbox\n",
        "            height = y2 - y\n",
        "            width = x2 - x\n",
        "            # proxy for confidence score\n",
        "            conf = np.mean(pred[y:y+height, x:x+width])\n",
        "            # add to predictionString\n",
        "            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n",
        "        # add filename and predictionString to dictionary\n",
        "        filename = filename.split('.')[0]\n",
        "        submission_dict[filename] = predictionString\n",
        "    # stop if we've got them all\n",
        "    if len(submission_dict) >= len(test_filenames):\n",
        "        break\n",
        "\n",
        "print(\"Done predicting!\")        \n",
        "        \n",
        "# save dictionary as csv file\n",
        "sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n",
        "sub.index.names = ['patientId']\n",
        "sub.columns = ['PredictionString']\n",
        "sub.to_csv('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1IxkJela_ql-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c rsna-pneumonia-detection-challenge -f submission.csv -m \"Colab segmentation 5 320x320 15 epochs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lf8l38RrA1tN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# upload checkpoint to GCS\n",
        "project_id = 'mammography-198911'\n",
        "bucket_name = 'pneumonia'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil cp ./{CHECKPOINT_PATH} gs://{bucket_name}/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}