{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-segmentation-connected-components.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "sliy7m7L-M-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Approach\n",
        "\n",
        "* Firstly a convolutional neural network is used to segment the image, using the bounding boxes directly as a mask. \n",
        "* Secondly connected components is used to separate multiple nodules.\n",
        "* Finally a bounding box is simply drawn around every connected component.\n",
        "\n",
        "# Network\n",
        "\n",
        "* The network consists of a number of residual blocks with convolutions and downsampling blocks with max pooling.\n",
        "* At the end of the network a single upsampling layer converts the output to the same shape as the input.\n",
        "\n",
        "As the input to the network is 256 by 256 (instead of the original 1024 by 1024) and the network downsamples a number of times without any meaningful upsampling (the final upsampling is just to match in 256 by 256 mask) the final prediction is very crude. If the network downsamples 4 times the final bounding boxes can only change with at least 16 pixels."
      ]
    },
    {
      "metadata": {
        "id": "xjzfSdjV-Y3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug \n",
        "!pip3 install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pu1pks8K-M-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import measure\n",
        "from skimage.transform import resize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFNwjWWc_JgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "def save_file_to_drive(name, path):\n",
        "  file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "  media = MediaFileUpload(path, \n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "  created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "  print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "  return created"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejrE2hQw-fMG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# enter your Kaggle credentionals here\n",
        "os.environ['KAGGLE_USERNAME']=\"skooch\"\n",
        "os.environ['KAGGLE_KEY']=\"42f8a02ee92cc773d1dbe66565673ad3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJskmI-Y-fwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath('./data')\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, 'logs')\n",
        "\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    os.makedirs(ROOT_DIR)\n",
        "os.chdir(ROOT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-tBhkYuWKWic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5710c384-3b15-4e37-dedd-155ca8c1ba55"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t\t\t       stage_1_test_images.zip\r\n",
            "GCP%20Credits%20Request%20Link%20-%20RSNA.txt  stage_1_train_images\r\n",
            "stage_1_detailed_class_info.csv.zip\t       stage_1_train_images.zip\r\n",
            "stage_1_sample_submission.csv\t\t       stage_1_train_labels.csv\r\n",
            "stage_1_test_images\t\t\t       stage_1_train_labels.csv.zip\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NonqWakT-jTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a4e151f7-9180-433c-f14d-0994b7432751"
      },
      "cell_type": "code",
      "source": [
        "# If you are unable to download the competition dataset, check to see if you have \n",
        "# accepted the user agreement on the competition website. \n",
        "!kaggle competitions download -c rsna-pneumonia-detection-challenge"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GCP%20Credits%20Request%20Link%20-%20RSNA.txt: Skipping, found more recently modified local copy (use --force to force download)\n",
            "^C\n",
            "User cancelled operation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YHvOAY1M-pzE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unzipping takes a few minutes\n",
        "!unzip -q -o stage_1_test_images.zip -d stage_1_test_images\n",
        "!unzip -q -o stage_1_train_images.zip -d stage_1_train_images\n",
        "!unzip -q -o stage_1_train_labels.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQG4ghkH-M-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load nodule locations\n",
        "\n",
        "Table contains [filename : nodule location] pairs per row. \n",
        "* If a filename contains multiple nodules, the table contains multiple rows with the same filename but different nodule locations. \n",
        "* If a filename contains no nodules it contains a single row with an empty nodule location.\n",
        "\n",
        "The code below loads the table and transforms it into a dictionary. \n",
        "* The dictionary uses the filename as key and a list of nodule locations in that filename as value. \n",
        "* If a filename is not present in the dictionary it means that it contains no nodules."
      ]
    },
    {
      "metadata": {
        "id": "67McqwYt-t5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82862f37-6910-4a0e-9116-44992705e518"
      },
      "cell_type": "code",
      "source": [
        "train_dicom_dir = os.path.join(ROOT_DIR, 'stage_1_train_images')\n",
        "test_dicom_dir = os.path.join(ROOT_DIR, 'stage_1_test_images')\n",
        "print(\"Train dir:\", train_dicom_dir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dir: /content/data/stage_1_train_images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HIYblxy8-M-f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# empty dictionary\n",
        "nodule_locations = {}\n",
        "# load table\n",
        "with open('./stage_1_train_labels.csv', mode='r') as infile:\n",
        "    # open reader\n",
        "    reader = csv.reader(infile)\n",
        "    # skip header\n",
        "    next(reader, None)\n",
        "    # loop through rows\n",
        "    for rows in reader:\n",
        "        # retrieve information\n",
        "        filename = rows[0]\n",
        "        location = rows[1:5]\n",
        "        nodule = rows[5]\n",
        "        # if row contains a nodule add label to dictionary\n",
        "        # which contains a list of nodule locations per filename\n",
        "        if nodule == '1':\n",
        "            # convert string to float to int\n",
        "            location = [int(float(i)) for i in location]\n",
        "            # save nodule location in dictionary\n",
        "            if filename in nodule_locations:\n",
        "                nodule_locations[filename].append(location)\n",
        "            else:\n",
        "                nodule_locations[filename] = [location]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "guL6FY9N-M-m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load filenames"
      ]
    },
    {
      "metadata": {
        "id": "ebHtuAkP-M-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec590178-1558-4286-837c-fbdafe30cc38"
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "folder = train_dicom_dir\n",
        "filenames = os.listdir(folder)\n",
        "random.shuffle(filenames)\n",
        "# split into train and validation filenames\n",
        "n_valid_samples = 2560\n",
        "train_filenames = filenames[n_valid_samples:]\n",
        "valid_filenames = filenames[:n_valid_samples]\n",
        "print('n train samples', len(train_filenames))\n",
        "print('n valid samples', len(valid_filenames))\n",
        "n_train_samples = len(filenames) - n_valid_samples"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n train samples 23124\n",
            "n valid samples 2560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRTP_RT6-M-7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # Data generator\n",
        "\n",
        "The dataset is too large to fit into memory, so we need to create a generator that loads data on the fly.\n",
        "\n",
        "* The generator takes in some filenames, batch_size and other parameters.\n",
        "\n",
        "* The generator outputs a random batch of numpy images and numpy masks.\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "mNHOs4vXGdwJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "IMAGE_SIZE = 320\n",
        "CHECKPOINT_PATH = \"model.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD51VT0I-M-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class generator(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, folder, filenames, nodule_locations=None, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=True, predict=False):\n",
        "        self.folder = folder\n",
        "        self.filenames = filenames\n",
        "        self.nodule_locations = nodule_locations\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.shuffle = shuffle\n",
        "        self.predict = predict\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __load__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # create empty mask\n",
        "        msk = np.zeros(img.shape)\n",
        "        # get filename without extension\n",
        "        filename = filename.split('.')[0]\n",
        "        # if image contains nodules\n",
        "        if filename in nodule_locations:\n",
        "            # loop through nodules\n",
        "            for location in nodule_locations[filename]:\n",
        "                # add 1's at the location of the nodule\n",
        "                x, y, w, h = location\n",
        "                msk[y:y+h, x:x+w] = 1\n",
        "        # resize both image and mask\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        msk = np.expand_dims(msk, -1)\n",
        "        return img, msk\n",
        "    \n",
        "    def __loadpredict__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # resize image\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        return img\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # select batch\n",
        "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # predict mode: return images and filenames\n",
        "        if self.predict:\n",
        "            # load files\n",
        "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            return imgs, filenames\n",
        "        # train mode: return images and masks\n",
        "        else:\n",
        "            # load files\n",
        "            items = [self.__load__(filename) for filename in filenames]\n",
        "            # unzip images and masks\n",
        "            imgs, msks = zip(*items)\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            msks = np.array(msks)\n",
        "            return imgs, msks\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.filenames)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.predict:\n",
        "            # return everything\n",
        "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "        else:\n",
        "            # return full batches only\n",
        "            return int(len(self.filenames) / self.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbHHFLzb-M_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Network"
      ]
    },
    {
      "metadata": {
        "id": "rYHu1v6V-M_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_downsample(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.MaxPool2D(2)(x)\n",
        "    return x\n",
        "\n",
        "def create_resblock(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    return keras.layers.add([x, inputs])\n",
        "\n",
        "def create_network(input_size, channels, n_blocks=2, depth=4):\n",
        "    # input\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n",
        "    # residual blocks\n",
        "    for d in range(depth):\n",
        "        channels = channels * 2\n",
        "        x = create_downsample(channels, x)\n",
        "        for b in range(n_blocks):\n",
        "            x = create_resblock(channels, x)\n",
        "    # output\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    x = keras.layers.UpSampling2D(2**(depth //2))(x)\n",
        "    outputs = keras.layers.UpSampling2D(2**(depth//2))(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wsBfvg2Y-M_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train network\n"
      ]
    },
    {
      "metadata": {
        "id": "qkxSFJ9w-M_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0a6505e-a60b-4f12-d9d7-37cf68164578"
      },
      "cell_type": "code",
      "source": [
        "# define iou or jaccard loss function\n",
        "def iou_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
        "    return 1 - score\n",
        "\n",
        "# combine bce loss and iou loss\n",
        "def iou_bce_loss(y_true, y_pred):\n",
        "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
        "\n",
        "# mean iou as a metric\n",
        "def mean_iou(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
        "    smooth = tf.ones(tf.shape(intersect))\n",
        "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
        "\n",
        "# create network and compiler\n",
        "model = create_network(input_size=IMAGE_SIZE, channels=24, n_blocks=2, depth=4)\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=iou_bce_loss,\n",
        "              metrics=['accuracy', mean_iou])\n",
        "\n",
        "# cosine learning rate annealing\n",
        "def cosine_annealing(x):\n",
        "    lr = 0.003\n",
        "    epochs = 25\n",
        "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
        "  \n",
        "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "\n",
        "# create train and validation generators\n",
        "folder = train_dicom_dir\n",
        "train_gen = generator(folder, train_filenames, nodule_locations, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=True, predict=False)\n",
        "valid_gen = generator(folder, valid_filenames, nodule_locations, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, shuffle=False, predict=False)\n",
        "\n",
        "history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate, checkpoint], epochs=25, shuffle=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yMDSkSHm-M_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(131)\n",
        "plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
        "plt.legend()\n",
        "plt.subplot(132)\n",
        "plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
        "plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
        "plt.legend()\n",
        "plt.subplot(133)\n",
        "plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
        "plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dz9at3ZH-NAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict test images"
      ]
    },
    {
      "metadata": {
        "id": "cCMcFaVN-NAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "folder = test_dicom_dir\n",
        "test_filenames = os.listdir(folder)\n",
        "print('n test samples:', len(test_filenames))\n",
        "\n",
        "# create test generator with predict flag set to True\n",
        "test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=IMAGE_SIZE, shuffle=False, predict=True)\n",
        "\n",
        "# create submission dictionary\n",
        "submission_dict = {}\n",
        "# loop through testset\n",
        "for imgs, filenames in test_gen:\n",
        "    # predict batch of images\n",
        "    preds = model.predict(imgs)\n",
        "    # loop through batch\n",
        "    for pred, filename in zip(preds, filenames):\n",
        "        # resize predicted mask\n",
        "        pred = resize(pred, (1024, 1024), mode='reflect')\n",
        "        # threshold predicted mask\n",
        "        comp = pred[:, :, 0] > 0.5\n",
        "        # apply connected components\n",
        "        comp = measure.label(comp)\n",
        "        # apply bounding boxes\n",
        "        predictionString = ''\n",
        "        for region in measure.regionprops(comp):\n",
        "            # retrieve x, y, height and width\n",
        "            y, x, y2, x2 = region.bbox\n",
        "            height = y2 - y\n",
        "            width = x2 - x\n",
        "            # proxy for confidence score\n",
        "            conf = np.mean(pred[y:y+height, x:x+width])\n",
        "            # add to predictionString\n",
        "            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n",
        "        # add filename and predictionString to dictionary\n",
        "        filename = filename.split('.')[0]\n",
        "        submission_dict[filename] = predictionString\n",
        "    # stop if we've got them all\n",
        "    if len(submission_dict) >= len(test_filenames):\n",
        "        break\n",
        "\n",
        "# save dictionary as csv file\n",
        "sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n",
        "sub.index.names = ['patientId']\n",
        "sub.columns = ['PredictionString']\n",
        "sub.to_csv('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whgCcttS_SUa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c rsna-pneumonia-detection-challenge -f submission.csv -m \"Baseline from colab size 320 5 epochs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0UDLKjA_TZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_file_to_drive(\"submission.csv\", sample_submission_fp2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}